% !TeX root = ../main.tex

\chapter{总结与展望}


在这个项目当中，为了解决CPU在对大型数据集排序过程中所面临的瓶颈，我们尝试针对该过程设计加速器。面对多种多样的排序算法以及海量的可能的加速器架构，我们使用了架构搜索的方法：首先根据时间复杂度以及硬件友好程度来排除掉部分算法以及部分架构，达到缩小搜索空间的目的，然后针对性的设计算法的加速模块，再根据不同的并行度对这些加速模块进行组合和验证测试，从而在最后搜索出最优的排序加速架构——16进制基数排序+传统64路归并排序架构，并将其运用在了加速构建八叉树的过程当中，将建树过程缩短了87\%，大幅提升了效率。

在设计排序模块的时候，我们也采用了很多创新性的手段。首先，在设计基数排序算法加速模块的过程中，我们创新性地采用了“统一桶”架构，大大节省了BRAM的使用，提升了空间利用效率，也解决了变上限循环无法优化的问题，将架构高度流水化；与此同时，我们还采用了Pingpong-buffer架构，通过两组“桶”直接通信，省去了中间数组的存入和读出的时间，进一步节省了排序时间。其次，在设计归并排序模块的时候，我们通过采用数组分割的手段，提升了架构的并行度。而在设计其他排序算法的加速模块的过程中，我们也运用了pragma进行了大量的优化。最后，我们采用HLS来进行硬件架构的设计，相较于编写verilog来说更加高效，体现了设计自动化的优越性。

然而，本工作也并非完美。首先，我们的搜索空间仍然很小，仅有13个架构；搜索的过程也是直接枚举评估，并没有采用当今架构搜索(NAS)常用的强化学习搜索、进化算法搜索等方法，实际上并没有利用到NAS思想的核心所在；实际上，这些搜索算法，也需要在含有大量架构的搜索空间中才可以体现优势，较小的搜索空间仍然限制了搜索自动化思想的应用；其次，由于1000万数据仍然能够在U280的BRAM当中存储，我们并没有设计与内存通信读取的模块，这限制了将我们的架构运用到更大的数据集上的可能性，因为BRAM一定是有存储上限的。最后，我们的架构相对于CPU来说，加速比仍然不够多。对于较底层的变成语言（如C语言），我们也仅仅能够达到4倍左右的加速比。这说明我们的架构仍然具有优化的空间。

基于目前的研究进展，我们觉得在未来我们可以有以下研究方向：
\begin{enumerate}
    \item 增加硬件架构的搜索空间，尝试囊括更多先进的排序算法如Timsort, Bitonic sort等，并尝试利用NAS的快速搜索方法，提升搜索的效率与精度；
    \item 尝试设计适用更大型数据集（如包含1亿数据的数据集）的加速架构，这就要求我们需要考虑与内存的通信，以及可能需要分块排序，重复从内存中加载数据；
    \item 尝试将我们的架构与GPU进行比较，对于GPU来说，其并行度更高，对于我们架构的优化会是更高层次的要求；
    \item 尝试运用我们的架构来解决更多实际问题，如碰撞检测，光线追踪，大规模地理数据的处理等方面。
\end{enumerate}

对于大型数据集排序的加速具有很高的实际意义与应用价值，对于这一方面的研究仍然有很多值得令人深究的方向，希望在未来能够基于目前的研究成果，在这一方向做出一定的贡献。